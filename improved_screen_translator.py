#!/usr/bin/env python3
import cv2
import easyocr
import openai
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import os
from pathlib import Path
import json
import time
import ffmpeg

class ImprovedScreenTextTranslator:
    def __init__(self, openai_api_key=None):
        print("ğŸ”§ ê°œì„ ëœ OCR ë° ë²ˆì—­ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # OCR ì´ˆê¸°í™” (ì¼ë³¸ì–´ + ì˜ì–´, ë” ì •í™•í•œ ì„¤ì •)
        self.ocr_reader = easyocr.Reader(['ja', 'en'], gpu=False)
        print("âœ… EasyOCR ì´ˆê¸°í™” ì™„ë£Œ")
        
        # OpenAI ì„¤ì •
        if openai_api_key:
            openai.api_key = openai_api_key
            print("âœ… OpenAI API ì—°ê²° ì™„ë£Œ")
        else:
            print("âš ï¸  OpenAI API í‚¤ê°€ ì—†ì–´ ë²ˆì—­ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    
    def extract_frames_from_segment(self, video_path, start_time=None, end_time=None, interval_seconds=10):
        """ë¹„ë””ì˜¤ êµ¬ê°„ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ"""
        if start_time is not None and end_time is not None:
            print(f"ğŸ¬ ë¹„ë””ì˜¤ êµ¬ê°„ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {start_time}ì´ˆ~{end_time}ì´ˆ ({interval_seconds}ì´ˆ ê°„ê²©)")
        else:
            print(f"ğŸ¬ ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {interval_seconds}ì´ˆ ê°„ê²©")
        
        cap = cv2.VideoCapture(video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        # êµ¬ê°„ ì„¤ì •
        if start_time is not None and end_time is not None:
            start_frame = int(start_time * fps)
            end_frame = int(end_time * fps)
        else:
            start_frame = 0
            end_frame = total_frames
            start_time = 0
            end_time = duration
        
        frames_data = []
        frame_interval = fps * interval_seconds
        
        for frame_num in range(start_frame, end_frame, frame_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
            ret, frame = cap.read()
            
            if not ret:
                break
                
            timestamp = frame_num / fps
            frames_data.append({
                'frame_number': frame_num,
                'timestamp': timestamp,
                'frame': frame
            })
            
            print(f"ğŸ“¸ í”„ë ˆì„ ì¶”ì¶œ: {timestamp:.1f}ì´ˆ")
        
        cap.release()
        print(f"âœ… ì´ {len(frames_data)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")
        return frames_data
    
    def enhance_frame_for_ocr(self, frame):
        """OCR ì •í™•ë„ë¥¼ ìœ„í•œ í”„ë ˆì„ ì „ì²˜ë¦¬"""
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # í•´ìƒë„ ì—…ìŠ¤ì¼€ì¼ë§ (2ë°°)
        height, width = gray.shape
        enhanced = cv2.resize(gray, (width*2, height*2), interpolation=cv2.INTER_CUBIC)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
        
        # ëŒ€ë¹„ ê°œì„ 
        enhanced = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(enhanced)
        
        return enhanced
    
    def extract_japanese_text_improved(self, frame):
        """ê°œì„ ëœ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # í”„ë ˆì„ ì „ì²˜ë¦¬
            enhanced_frame = self.enhance_frame_for_ocr(frame)
            
            # OCR ì‹¤í–‰ (ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ í…ìŠ¤íŠ¸ ê°ì§€)
            results = self.ocr_reader.readtext(
                enhanced_frame, 
                detail=1,
                paragraph=False,
                width_ths=0.5,    # ë” ë„“ì€ í­ í—ˆìš©
                height_ths=0.5,   # ë” ë„“ì€ ë†’ì´ í—ˆìš©
                text_threshold=0.5,  # ë‚®ì€ í…ìŠ¤íŠ¸ ì„ê³„ê°’
                low_text=0.3      # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
            )
            
            japanese_texts = []
            for (bbox, text, confidence) in results:
                # ë” ë‚®ì€ ì‹ ë¢°ë„ë„ í—ˆìš© (50% ì´ìƒ)
                if confidence > 0.5:
                    # ì¼ë³¸ì–´ ë¬¸ìê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
                    if self.contains_japanese(text):
                        # ìŠ¤ì¼€ì¼ë§ ë³´ì • (2ë°° ì—…ìŠ¤ì¼€ì¼í–ˆìœ¼ë¯€ë¡œ ì¢Œí‘œë¥¼ ë°˜ìœ¼ë¡œ)
                        corrected_bbox = [[point[0]/2, point[1]/2] for point in bbox]
                        
                        japanese_texts.append({
                            'text': text.strip(),
                            'bbox': corrected_bbox,
                            'confidence': confidence
                        })
            
            return japanese_texts
            
        except Exception as e:
            print(f"âŒ OCR ì˜¤ë¥˜: {e}")
            return []
    
    def contains_japanese(self, text):
        """í…ìŠ¤íŠ¸ì— ì¼ë³¸ì–´ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ê°œì„ ëœ ê°ì§€)"""
        japanese_ranges = [
            (0x3040, 0x309F),  # íˆë¼ê°€ë‚˜
            (0x30A0, 0x30FF),  # ê°€íƒ€ì¹´ë‚˜  
            (0x4E00, 0x9FAF),  # í•œì
            (0xFF65, 0xFF9F),  # ë°˜ê° ê°€íƒ€ì¹´ë‚˜
        ]
        
        japanese_count = 0
        total_chars = len(text.strip())
        
        if total_chars == 0:
            return False
            
        for char in text:
            char_code = ord(char)
            for start, end in japanese_ranges:
                if start <= char_code <= end:
                    japanese_count += 1
                    break
        
        # ë¬¸ìì˜ 30% ì´ìƒì´ ì¼ë³¸ì–´ë©´ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
        return japanese_count / total_chars >= 0.3
    
    def translate_texts_improved(self, texts):
        """ê°œì„ ëœ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ë²ˆì—­ (ë” ê°„ê²°í•˜ê²Œ)"""
        if not openai.api_key or not texts:
            return [f"[ë²ˆì—­ í•„ìš”] {text}" for text in texts]
        
        print(f"ğŸŒ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ë²ˆì—­ ì¤‘...")
        
        try:
            # ë°°ì¹˜ ë²ˆì—­
            batch_text = "\n---\n".join(texts)
            
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ì „ë¬¸ ì¼ë³¸ì–´-í•œêµ­ì–´ ë²ˆì—­ê°€ì…ë‹ˆë‹¤.
                        í™”ë©´ì— í‘œì‹œëœ ìŠ¬ë¼ì´ë“œ/UI í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
                        ë§¤ìš° ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
                        ê¸°ìˆ  ìš©ì–´, íšŒì‚¬ëª…, ê³ ìœ ëª…ì‚¬ëŠ” ì ì ˆíˆ ìœ ì§€í•˜ë˜ ì½ê¸° ì‰½ê²Œ í•˜ì„¸ìš”.
                        ê¸´ ë¬¸ì¥ì€ í•µì‹¬ë§Œ ê°„ë‹¨íˆ ë²ˆì—­í•˜ì„¸ìš”.
                        ê° í…ìŠ¤íŠ¸ëŠ” '---'ë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤."""
                    },
                    {
                        "role": "user",
                        "content": f"ë‹¤ìŒ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë“¤ì„ ê°„ê²°í•œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”:\n\n{batch_text}"
                    }
                ]
            )
            
            translated_batch = response.choices[0].message.content
            translations = translated_batch.split("\n---\n")
            
            # ê°œìˆ˜ ë§ì¶”ê¸°
            if len(translations) != len(texts):
                print("âš ï¸  ë²ˆì—­ ê°œìˆ˜ ë¶ˆì¼ì¹˜, ê°œë³„ ë²ˆì—­ìœ¼ë¡œ ì¬ì‹œë„...")
                return [self.translate_single_text(text) for text in texts]
            
            print(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translations)}ê°œ")
            return translations
            
        except Exception as e:
            print(f"âŒ ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return [f"[ë²ˆì—­ ì‹¤íŒ¨] {text}" for text in texts]
    
    def translate_single_text(self, text):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë²ˆì—­"""
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ì¼ë³¸ì–´ë¥¼ ê°„ê²°í•œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”."},
                    {"role": "user", "content": text}
                ]
            )
            return response.choices[0].message.content
        except:
            return f"[ë²ˆì—­ ì‹¤íŒ¨] {text}"
    
    def create_improved_overlay_frame(self, original_frame, text_data_list):
        """ê°œì„ ëœ ì˜¤ë²„ë ˆì´ í”„ë ˆì„ ìƒì„± (ì‘ì€ í°íŠ¸, ê¹”ë”í•œ ë””ìì¸)"""
        height, width = original_frame.shape[:2]
        
        # PILë¡œ ë³€í™˜í•˜ì—¬ í•œê¸€ í°íŠ¸ ì²˜ë¦¬
        pil_image = Image.fromarray(cv2.cvtColor(original_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image)
        
        # ì‘ì€ í•œê¸€ í°íŠ¸ ë¡œë“œ
        try:
            font_paths = [
                "/System/Library/Fonts/AppleSDGothicNeo.ttc",
                "/System/Library/Fonts/Helvetica.ttc",
                "/Library/Fonts/Arial Unicode MS.ttf"
            ]
            
            font = None
            for font_path in font_paths:
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, 12)  # ì‘ì€ í°íŠ¸ í¬ê¸°
                    break
            
            if font is None:
                font = ImageFont.load_default()
                
        except:
            font = ImageFont.load_default()
        
        for text_data in text_data_list:
            bbox = text_data['bbox']
            korean_text = text_data['korean_text']
            
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¤„ì„
            if len(korean_text) > 20:
                korean_text = korean_text[:18] + "..."
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚°
            x_coords = [point[0] for point in bbox]
            y_coords = [point[1] for point in bbox]
            x_min, x_max = min(x_coords), max(x_coords)
            y_min, y_max = min(y_coords), max(y_coords)
            
            # ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ ë°”ë¡œ ì•„ë˜ìª½ì— ì‘ê²Œ ë°°ì¹˜
            text_y = y_max + 2
            
            # ì‘ì€ ë°°ê²½ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
            text_bbox = draw.textbbox((x_min, text_y), korean_text, font=font)
            padding = 2
            bg_bbox = [
                text_bbox[0] - padding,
                text_bbox[1] - padding, 
                text_bbox[2] + padding,
                text_bbox[3] + padding
            ]
            
            # ë°˜íˆ¬ëª… ë°°ê²½
            overlay = Image.new('RGBA', pil_image.size, (0, 0, 0, 0))
            overlay_draw = ImageDraw.Draw(overlay)
            overlay_draw.rectangle(bg_bbox, fill=(0, 0, 0, 120))
            
            # ë°°ê²½ í•©ì„±
            pil_image = Image.alpha_composite(pil_image.convert('RGBA'), overlay).convert('RGB')
            draw = ImageDraw.Draw(pil_image)
            
            # ì‘ì€ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (í°ìƒ‰)
            draw.text((x_min, text_y), korean_text, font=font, fill=(255, 255, 255))
        
        # PILì—ì„œ OpenCVë¡œ ë‹¤ì‹œ ë³€í™˜
        result_frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        return result_frame
    
    def process_video_segment_improved(self, video_path, output_path, start_time=None, end_time=None, interval_seconds=10):
        """ê°œì„ ëœ ë¹„ë””ì˜¤ êµ¬ê°„ ì²˜ë¦¬"""
        segment_info = f"_{start_time}s-{end_time}s" if start_time is not None and end_time is not None else ""
        print(f"ğŸš€ ê°œì„ ëœ í™”ë©´ í…ìŠ¤íŠ¸ ë²ˆì—­ ì‹œì‘: {Path(video_path).name}{segment_info}")
        
        # 1. í”„ë ˆì„ ì¶”ì¶œ
        frames_data = self.extract_frames_from_segment(video_path, start_time, end_time, interval_seconds)
        
        # 2. ê° í”„ë ˆì„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë²ˆì—­
        processed_frames = []
        
        for i, frame_data in enumerate(frames_data):
            print(f"\nğŸ“ í”„ë ˆì„ {i+1}/{len(frames_data)} ì²˜ë¦¬ ì¤‘ ({frame_data['timestamp']:.1f}ì´ˆ)")
            
            frame = frame_data['frame']
            
            # ê°œì„ ëœ OCRë¡œ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            japanese_texts = self.extract_japanese_text_improved(frame)
            
            if japanese_texts:
                print(f"   ğŸ“– ì¶”ì¶œëœ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸: {len(japanese_texts)}ê°œ")
                
                # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë²ˆì—­
                texts_to_translate = [item['text'] for item in japanese_texts]
                korean_translations = self.translate_texts_improved(texts_to_translate)
                
                # ë²ˆì—­ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€
                text_data_list = []
                for j, japanese_text_data in enumerate(japanese_texts):
                    text_data_list.append({
                        'bbox': japanese_text_data['bbox'],
                        'japanese_text': japanese_text_data['text'],
                        'korean_text': korean_translations[j] if j < len(korean_translations) else "[ë²ˆì—­ ì‹¤íŒ¨]",
                        'confidence': japanese_text_data['confidence']
                    })
                
                # ê°œì„ ëœ ì˜¤ë²„ë ˆì´ í”„ë ˆì„ ìƒì„±
                overlay_frame = self.create_improved_overlay_frame(frame, text_data_list)
                
                processed_frames.append({
                    'timestamp': frame_data['timestamp'],
                    'frame': overlay_frame,
                    'has_translation': True
                })
                
                print(f"   âœ… ë²ˆì—­ ì˜¤ë²„ë ˆì´ ì™„ë£Œ")
                
            else:
                print(f"   âšª ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ì—†ìŒ")
                processed_frames.append({
                    'timestamp': frame_data['timestamp'],
                    'frame': frame,
                    'has_translation': False
                })
        
        # 3. ì²˜ë¦¬ëœ í”„ë ˆì„ë“¤ë¡œ ë¹„ë””ì˜¤ ìƒì„±
        self.create_video_from_processed_frames_improved(video_path, processed_frames, output_path, start_time, end_time)
        
        return output_path
    
    def create_video_from_processed_frames_improved(self, original_video_path, processed_frames, output_path, start_time=None, end_time=None):
        """ê°œì„ ëœ ë¹„ë””ì˜¤ ìƒì„±"""
        print("ğŸ¬ ê°œì„ ëœ ë²ˆì—­ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
        
        # ì›ë³¸ ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cap = cv2.VideoCapture(original_video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # êµ¬ê°„ ì„¤ì •
        if start_time is not None and end_time is not None:
            start_frame = int(start_time * fps)
            end_frame = int(end_time * fps)
        else:
            start_frame = 0
            end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        total_frames = end_frame - start_frame
        
        # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # í”„ë ˆì„ë³„ ì˜¤ë²„ë ˆì´ ë§µ ìƒì„±
        overlay_map = {}
        for pframe in processed_frames:
            frame_num = int(pframe['timestamp'] * fps) - start_frame
            overlay_map[frame_num] = pframe
        
        # ì›ë³¸ ë¹„ë””ì˜¤ êµ¬ê°„ì„ ìˆœíšŒí•˜ë©° ì˜¤ë²„ë ˆì´ ì ìš©
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        
        current_overlay = None
        for frame_num in range(total_frames):
            ret, frame = cap.read()
            if not ret:
                break
            
            # í˜„ì¬ í”„ë ˆì„ì— ì ìš©í•  ì˜¤ë²„ë ˆì´ í™•ì¸
            if frame_num in overlay_map:
                current_overlay = overlay_map[frame_num]
            
            # ì˜¤ë²„ë ˆì´ ì ìš© (10ì´ˆê°„ ìœ ì§€)
            if current_overlay and current_overlay['has_translation']:
                output_frame = current_overlay['frame']
            else:
                output_frame = frame
            
            out.write(output_frame)
            
            if frame_num % (fps * 5) == 0:  # 5ì´ˆë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                progress = (frame_num / total_frames) * 100
                print(f"   ğŸ“¹ ì²˜ë¦¬ ì¤‘: {progress:.1f}%")
        
        cap.release()
        out.release()
        
        print(f"âœ… ê°œì„ ëœ ë²ˆì—­ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")

def main():
    # OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("export OPENAI_API_KEY=your_api_key_here")
        return
    
    # ì›ë³¸ ë¹„ë””ì˜¤ ì‚¬ìš©
    video_path = "/Users/smgu/test_code/video_converter/video.mov"
    output_path = "/Users/smgu/test_code/video_converter/video_improved_screen_translation.mp4"
    
    # ì¤‘ê°„ êµ¬ê°„ 1ë¶„ í…ŒìŠ¤íŠ¸ (30ë¶„~31ë¶„)
    start_time = 30 * 60  # 30ë¶„
    end_time = 31 * 60    # 31ë¶„
    
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ êµ¬ê°„: {start_time//60}ë¶„{start_time%60:02d}ì´ˆ ~ {end_time//60}ë¶„{end_time%60:02d}ì´ˆ")
    
    # ê°œì„ ëœ í™”ë©´ í…ìŠ¤íŠ¸ ë²ˆì—­ê¸° ì‹¤í–‰
    translator = ImprovedScreenTextTranslator(api_key)
    result = translator.process_video_segment_improved(
        video_path, 
        output_path, 
        start_time=start_time,
        end_time=end_time,
        interval_seconds=10  # 10ì´ˆë§ˆë‹¤ í™”ë©´ í…ìŠ¤íŠ¸ í™•ì¸
    )
    
    print(f"\nğŸ‰ ê°œì„ ëœ í™”ë©´ ë²ˆì—­ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {result}")

if __name__ == "__main__":
    main()