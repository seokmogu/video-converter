#!/usr/bin/env python3
import cv2
import easyocr
import openai
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import os
from pathlib import Path
import json
import time

class ScreenTextTranslator:
    def __init__(self, openai_api_key=None, source_language=None, target_language=None, ocr_languages=None):
        print("ğŸ”§ OCR ë° ë²ˆì—­ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ì–¸ì–´ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ì½ê¸°)
        self.source_language = source_language or os.getenv('SOURCE_LANGUAGE', 'ja')
        self.target_language = target_language or os.getenv('TARGET_LANGUAGE', 'Korean')
        
        # OCR ì–¸ì–´ ì„¤ì •
        ocr_lang_str = ocr_languages or os.getenv('OCR_LANGUAGES', 'ja,en')
        ocr_lang_list = [lang.strip() for lang in ocr_lang_str.split(',')]
        
        # OCR ì´ˆê¸°í™”
        self.ocr_reader = easyocr.Reader(ocr_lang_list)
        print(f"âœ… EasyOCR ì´ˆê¸°í™” ì™„ë£Œ: {ocr_lang_list}")
        
        # OpenAI ì„¤ì •
        if openai_api_key:
            openai.api_key = openai_api_key
            print("âœ… OpenAI API ì—°ê²° ì™„ë£Œ")
        else:
            print("âš ï¸  OpenAI API í‚¤ê°€ ì—†ì–´ ë²ˆì—­ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        
        print(f"ğŸŒ ì–¸ì–´ ì„¤ì •: {self.source_language} â†’ {self.target_language}")
    
    def extract_frames_with_text_change(self, video_path, interval_seconds=5):
        """ë¹„ë””ì˜¤ì—ì„œ ì¼ì • ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œ"""
        print(f"ğŸ¬ ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {interval_seconds}ì´ˆ ê°„ê²©")
        
        cap = cv2.VideoCapture(video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        frames_data = []
        frame_interval = fps * interval_seconds
        
        for frame_num in range(0, total_frames, frame_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
            ret, frame = cap.read()
            
            if not ret:
                break
                
            timestamp = frame_num / fps
            frames_data.append({
                'frame_number': frame_num,
                'timestamp': timestamp,
                'frame': frame
            })
            
            print(f"ğŸ“¸ í”„ë ˆì„ ì¶”ì¶œ: {timestamp:.1f}ì´ˆ")
        
        cap.release()
        print(f"âœ… ì´ {len(frames_data)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")
        return frames_data
    
    def extract_japanese_text_from_frame(self, frame):
        """í”„ë ˆì„ì—ì„œ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # OCR ì‹¤í–‰
            results = self.ocr_reader.readtext(frame, detail=1)
            
            japanese_texts = []
            for (bbox, text, confidence) in results:
                # ì‹ ë¢°ë„ 70% ì´ìƒë§Œ ì‚¬ìš©
                if confidence > 0.7:
                    # ì¼ë³¸ì–´ ë¬¸ìê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
                    if self.contains_japanese(text):
                        japanese_texts.append({
                            'text': text,
                            'bbox': bbox,
                            'confidence': confidence
                        })
            
            return japanese_texts
            
        except Exception as e:
            print(f"âŒ OCR ì˜¤ë¥˜: {e}")
            return []
    
    def contains_japanese(self, text):
        """í…ìŠ¤íŠ¸ì— ì¼ë³¸ì–´ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        japanese_ranges = [
            (0x3040, 0x309F),  # íˆë¼ê°€ë‚˜
            (0x30A0, 0x30FF),  # ê°€íƒ€ì¹´ë‚˜  
            (0x4E00, 0x9FAF),  # í•œì
        ]
        
        for char in text:
            char_code = ord(char)
            for start, end in japanese_ranges:
                if start <= char_code <= end:
                    return True
        return False
    
    def translate_texts(self, texts):
        """ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
        if not openai.api_key or not texts:
            return [f"[ë²ˆì—­ í•„ìš”] {text}" for text in texts]
        
        print(f"ğŸŒ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ë²ˆì—­ ì¤‘...")
        
        try:
            # ë°°ì¹˜ ë²ˆì—­
            batch_text = "\n---\n".join(texts)
            
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ì „ë¬¸ ì¼ë³¸ì–´-í•œêµ­ì–´ ë²ˆì—­ê°€ì…ë‹ˆë‹¤.
                        í™”ë©´ì— í‘œì‹œëœ ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
                        ê¸°ìˆ  ìš©ì–´, íšŒì‚¬ëª…, ê³ ìœ ëª…ì‚¬ëŠ” ì ì ˆíˆ ìœ ì§€í•˜ë˜ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
                        ê° í…ìŠ¤íŠ¸ëŠ” '---'ë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤."""
                    },
                    {
                        "role": "user",
                        "content": f"ë‹¤ìŒ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”:\n\n{batch_text}"
                    }
                ]
            )
            
            translated_batch = response.choices[0].message.content
            translations = translated_batch.split("\n---\n")
            
            # ê°œìˆ˜ ë§ì¶”ê¸°
            if len(translations) != len(texts):
                print("âš ï¸  ë²ˆì—­ ê°œìˆ˜ ë¶ˆì¼ì¹˜, ê°œë³„ ë²ˆì—­ìœ¼ë¡œ ì¬ì‹œë„...")
                return [self.translate_single_text(text) for text in texts]
            
            print(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translations)}ê°œ")
            return translations
            
        except Exception as e:
            print(f"âŒ ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return [f"[ë²ˆì—­ ì‹¤íŒ¨] {text}" for text in texts]
    
    def translate_single_text(self, text):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë²ˆì—­"""
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ì¼ë³¸ì–´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”."},
                    {"role": "user", "content": text}
                ]
            )
            return response.choices[0].message.content
        except:
            return f"[ë²ˆì—­ ì‹¤íŒ¨] {text}"
    
    def create_overlay_frame(self, original_frame, text_data_list):
        """ì›ë³¸ í”„ë ˆì„ì— ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë²„ë ˆì´"""
        height, width = original_frame.shape[:2]
        
        # ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´ ìƒì„±
        overlay = original_frame.copy()
        overlay_alpha = 0.7
        
        # PILë¡œ ë³€í™˜í•˜ì—¬ í•œê¸€ í°íŠ¸ ì²˜ë¦¬
        pil_image = Image.fromarray(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image)
        
        # í•œê¸€ í°íŠ¸ ë¡œë“œ ì‹œë„
        try:
            # macOS ê¸°ë³¸ í•œê¸€ í°íŠ¸ë“¤ ì‹œë„
            font_paths = [
                "/System/Library/Fonts/AppleSDGothicNeo.ttc",
                "/System/Library/Fonts/Helvetica.ttc",
                "/Library/Fonts/Arial Unicode MS.ttf"
            ]
            
            font = None
            for font_path in font_paths:
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, 20)
                    break
            
            if font is None:
                font = ImageFont.load_default()
                
        except:
            font = ImageFont.load_default()
        
        for text_data in text_data_list:
            bbox = text_data['bbox']
            korean_text = text_data['korean_text']
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚°
            x_coords = [point[0] for point in bbox]
            y_coords = [point[1] for point in bbox]
            x_min, x_max = min(x_coords), max(x_coords)
            y_min, y_max = min(y_coords), max(y_coords)
            
            # ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ ì•„ë˜ìª½ì— ë°°ì¹˜
            text_y = y_max + 5
            
            # ë°°ê²½ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
            text_bbox = draw.textbbox((x_min, text_y), korean_text, font=font)
            draw.rectangle(text_bbox, fill=(0, 0, 0, 180))  # ë°˜íˆ¬ëª… ê²€ì€ ë°°ê²½
            
            # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            draw.text((x_min, text_y), korean_text, font=font, fill=(255, 255, 255))
        
        # PILì—ì„œ OpenCVë¡œ ë‹¤ì‹œ ë³€í™˜
        result_frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        return result_frame
    
    def process_video_with_translation(self, video_path, output_path, interval_seconds=5):
        """ë¹„ë””ì˜¤ ì „ì²´ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²ˆì—­ ì˜¤ë²„ë ˆì´ ì¶”ê°€"""
        print(f"ğŸš€ í™”ë©´ í…ìŠ¤íŠ¸ ë²ˆì—­ ë¹„ë””ì˜¤ ìƒì„± ì‹œì‘: {Path(video_path).name}")
        
        # 1. í”„ë ˆì„ ì¶”ì¶œ
        frames_data = self.extract_frames_with_text_change(video_path, interval_seconds)
        
        # 2. ê° í”„ë ˆì„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë²ˆì—­
        processed_frames = []
        
        for i, frame_data in enumerate(frames_data):
            print(f"\nğŸ“ í”„ë ˆì„ {i+1}/{len(frames_data)} ì²˜ë¦¬ ì¤‘ ({frame_data['timestamp']:.1f}ì´ˆ)")
            
            frame = frame_data['frame']
            
            # OCRë¡œ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            japanese_texts = self.extract_japanese_text_from_frame(frame)
            
            if japanese_texts:
                print(f"   ğŸ“– ì¶”ì¶œëœ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸: {len(japanese_texts)}ê°œ")
                
                # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë²ˆì—­
                texts_to_translate = [item['text'] for item in japanese_texts]
                korean_translations = self.translate_texts(texts_to_translate)
                
                # ë²ˆì—­ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€
                text_data_list = []
                for j, japanese_text_data in enumerate(japanese_texts):
                    text_data_list.append({
                        'bbox': japanese_text_data['bbox'],
                        'japanese_text': japanese_text_data['text'],
                        'korean_text': korean_translations[j] if j < len(korean_translations) else "[ë²ˆì—­ ì‹¤íŒ¨]",
                        'confidence': japanese_text_data['confidence']
                    })
                
                # ì˜¤ë²„ë ˆì´ í”„ë ˆì„ ìƒì„±
                overlay_frame = self.create_overlay_frame(frame, text_data_list)
                
                processed_frames.append({
                    'timestamp': frame_data['timestamp'],
                    'frame': overlay_frame,
                    'has_translation': True
                })
                
                print(f"   âœ… ë²ˆì—­ ì˜¤ë²„ë ˆì´ ì™„ë£Œ")
                
            else:
                print(f"   âšª ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ ì—†ìŒ")
                processed_frames.append({
                    'timestamp': frame_data['timestamp'],
                    'frame': frame,
                    'has_translation': False
                })
        
        # 3. ì²˜ë¦¬ëœ í”„ë ˆì„ë“¤ë¡œ ë¹„ë””ì˜¤ ìƒì„±
        self.create_video_from_processed_frames(video_path, processed_frames, output_path)
        
        return output_path
    
    def create_video_from_processed_frames(self, original_video_path, processed_frames, output_path):
        """ì²˜ë¦¬ëœ í”„ë ˆì„ë“¤ë¡œ ìƒˆ ë¹„ë””ì˜¤ ìƒì„±"""
        print("ğŸ¬ ë²ˆì—­ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
        
        # ì›ë³¸ ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cap = cv2.VideoCapture(original_video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # í”„ë ˆì„ë³„ ì˜¤ë²„ë ˆì´ ë§µ ìƒì„±
        overlay_map = {}
        for pframe in processed_frames:
            frame_num = int(pframe['timestamp'] * fps)
            overlay_map[frame_num] = pframe
        
        # ì›ë³¸ ë¹„ë””ì˜¤ë¥¼ ìˆœíšŒí•˜ë©° ì˜¤ë²„ë ˆì´ ì ìš©
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        
        current_overlay = None
        for frame_num in range(total_frames):
            ret, frame = cap.read()
            if not ret:
                break
            
            # í˜„ì¬ í”„ë ˆì„ì— ì ìš©í•  ì˜¤ë²„ë ˆì´ í™•ì¸
            if frame_num in overlay_map:
                current_overlay = overlay_map[frame_num]
            
            # ì˜¤ë²„ë ˆì´ ì ìš©
            if current_overlay and current_overlay['has_translation']:
                # 5ì´ˆê°„ ì˜¤ë²„ë ˆì´ ìœ ì§€ (ë‹¤ìŒ ì˜¤ë²„ë ˆì´ê¹Œì§€)
                output_frame = current_overlay['frame']
            else:
                output_frame = frame
            
            out.write(output_frame)
            
            if frame_num % (fps * 10) == 0:  # 10ì´ˆë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                print(f"   ğŸ“¹ ì²˜ë¦¬ ì¤‘: {frame_num/fps:.1f}ì´ˆ / {total_frames/fps:.1f}ì´ˆ")
        
        cap.release()
        out.release()
        
        print(f"âœ… ë²ˆì—­ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")

def main():
    # OpenAI API í‚¤ ì„¤ì •
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
        api_key = input().strip() or None
    
    # ì…ë ¥ ë¹„ë””ì˜¤ (ê¸°ì¡´ ìë§‰ í¬í•¨ ë¹„ë””ì˜¤)
    video_path = "/Users/smgu/test_code/video_converter/video_with_subtitles.mp4"
    output_path = "/Users/smgu/test_code/video_converter/video_full_translated.mp4"
    
    # í™”ë©´ í…ìŠ¤íŠ¸ ë²ˆì—­ê¸° ì‹¤í–‰
    translator = ScreenTextTranslator(api_key)
    result = translator.process_video_with_translation(
        video_path, 
        output_path, 
        interval_seconds=10  # 10ì´ˆë§ˆë‹¤ í™”ë©´ í…ìŠ¤íŠ¸ í™•ì¸
    )
    
    print(f"\nğŸ‰ ì™„ì „ ë²ˆì—­ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {result}")

if __name__ == "__main__":
    main()